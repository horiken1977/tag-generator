#!/usr/bin/env python3
"""
æ®µéšåˆ†é›¢å¼ã‚¿ã‚°å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ 

ç¬¬1æ®µéš: æ–‡å­—èµ·ã“ã—é™¤å¤–ã§å…¨ä»¶åˆ†æ â†’ ã‚¿ã‚°å€™è£œç”Ÿæˆ â†’ Webè¡¨ç¤º â†’ ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
ç¬¬2æ®µéš: æ‰¿èªæ¸ˆã¿ã‚¿ã‚°å€™è£œã§1ä»¶ãšã¤è©³ç´°åˆ†æ â†’ æœ€çµ‚ã‚¿ã‚°ä»˜ã‘
"""

import json
import logging
from typing import List, Dict, Any, Set
import re
from datetime import datetime

class StagedTagProcessor:
    """æ®µéšåˆ†é›¢å¼ã‚¿ã‚°å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, ai_handler=None):
        self.ai_handler = ai_handler
        self.logger = logging.getLogger(__name__)
        self.stage1_candidates = set()
        self.approved_candidates = set()
        
    def execute_stage1_candidate_generation(self, all_video_data: List[Dict[str, Any]], max_batch_size: int = 50) -> Dict[str, Any]:
        """
        ç¬¬1æ®µéš: æ–‡å­—èµ·ã“ã—é™¤å¤–ã§ã®å…¨ä»¶åˆ†æã¨ã‚¿ã‚°å€™è£œç”Ÿæˆ
        
        Args:
            all_video_data: å…¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            stage1çµæœï¼ˆã‚¿ã‚°å€™è£œã€çµ±è¨ˆæƒ…å ±ç­‰ï¼‰
        """
        # é«˜è² è·å¯¾ç­–: ãƒãƒƒãƒã‚µã‚¤ã‚ºåˆ¶é™
        if len(all_video_data) > max_batch_size:
            print(f"âš ï¸ é«˜è² è·å¯¾ç­–: {len(all_video_data)}ä»¶ â†’ æœ€åˆã®{max_batch_size}ä»¶ã®ã¿å‡¦ç†")
            all_video_data = all_video_data[:max_batch_size]
        
        print(f"\n{'='*60}")
        print(f"ç¬¬1æ®µéšé–‹å§‹: ã‚¿ã‚°å€™è£œç”Ÿæˆï¼ˆæ–‡å­—èµ·ã“ã—é™¤å¤–åˆ†æï¼‰")
        print(f"å¯¾è±¡å‹•ç”»æ•°: {len(all_video_data)}ä»¶")
        print(f"{'='*60}")
        
        start_time = datetime.now()
        
        # æ–‡å­—èµ·ã“ã—ä»¥å¤–ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„
        aggregated_data = self._aggregate_non_transcript_data(all_video_data)
        
        # ã‚¿ã‚°å€™è£œã‚’ç”Ÿæˆ
        self.stage1_candidates = self._generate_tag_candidates(aggregated_data)
        
        # å³æ ¼ãªæ±ç”¨ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
        filtered_candidates = self._apply_strict_generic_filter(list(self.stage1_candidates))
        
        # é¡ç¾©èªçµ±ä¸€å‡¦ç†ã‚’é©ç”¨
        unified_candidates = self._apply_synonym_unification(filtered_candidates)
        self.stage1_candidates = set(unified_candidates)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        result = {
            'stage': 1,
            'success': True,
            'tag_candidates': sorted(list(self.stage1_candidates)),
            'candidate_count': len(self.stage1_candidates),
            'processing_time': processing_time,
            'source_data_stats': {
                'total_videos': len(all_video_data),
                'titles_processed': len([v for v in all_video_data if v.get('title')]),
                'skills_processed': len([v for v in all_video_data if v.get('skill')]),
                'descriptions_processed': len([v for v in all_video_data if v.get('description')]),
                'summaries_processed': len([v for v in all_video_data if v.get('summary')]),
                'transcripts_excluded': True
            },
            'message': 'ã‚¿ã‚°å€™è£œãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚å†…å®¹ã‚’ç¢ºèªã—ã¦æ‰¿èªã—ã¦ãã ã•ã„ã€‚'
        }
        
        print(f"ç¬¬1æ®µéšå®Œäº†: {len(self.stage1_candidates)}å€‹ã®ã‚¿ã‚°å€™è£œã‚’ç”Ÿæˆ")
        print(f"å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"ã‚¿ã‚°å€™è£œä¾‹: {sorted(list(self.stage1_candidates))[:10]}...")
        
        return result
    
    def execute_stage2_individual_tagging(self, all_video_data: List[Dict[str, Any]], approved_candidates: List[str], ai_engine: str = 'openai') -> Dict[str, Any]:
        """
        ç¬¬2æ®µéš: æ‰¿èªã•ã‚ŒãŸã‚¿ã‚°å€™è£œã‚’ä½¿ç”¨ã—ã¦ã®1ä»¶ãšã¤è©³ç´°åˆ†æ
        
        Args:
            all_video_data: å…¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            approved_candidates: ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ‰¿èªã—ãŸã‚¿ã‚°å€™è£œã®ãƒªã‚¹ãƒˆ
            ai_engine: ä½¿ç”¨ã™ã‚‹AIã‚¨ãƒ³ã‚¸ãƒ³
            
        Returns:
            stage2çµæœï¼ˆå„å‹•ç”»ã®ã‚¿ã‚°ä»˜ã‘çµæœï¼‰
        """
        print(f"\n{'='*60}")
        print(f"ç¬¬2æ®µéšé–‹å§‹: å€‹åˆ¥å‹•ç”»ã‚¿ã‚°ä»˜ã‘ï¼ˆæ–‡å­—èµ·ã“ã—å«ã‚€è©³ç´°åˆ†æï¼‰")
        print(f"å¯¾è±¡å‹•ç”»æ•°: {len(all_video_data)}ä»¶")
        print(f"ä½¿ç”¨ã‚¿ã‚°å€™è£œæ•°: {len(approved_candidates)}å€‹")
        print(f"{'='*60}")
        
        if not approved_candidates:
            return {
                'stage': 2,
                'success': False,
                'error': 'ã‚¿ã‚°å€™è£œãŒæ‰¿èªã•ã‚Œã¦ã„ã¾ã›ã‚“',
                'results': []
            }
        
        self.approved_candidates = set(approved_candidates)
        start_time = datetime.now()
        
        # å„å‹•ç”»ã‚’å€‹åˆ¥ã«è©³ç´°åˆ†æ
        results = []
        for i, video in enumerate(all_video_data):
            print(f"\n--- å‹•ç”» {i+1}/{len(all_video_data)} ã‚’åˆ†æä¸­ ---")
            
            selected_tags = self._analyze_individual_video(video, ai_engine)
            
            result = {
                'video_index': i,
                'title': video.get('title', ''),
                'selected_tags': selected_tags,
                'tag_count': len(selected_tags),
                'confidence': self._calculate_confidence(selected_tags, video)
            }
            results.append(result)
            
            print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {video.get('title', 'Unknown')[:50]}...")
            print(f"  é¸å®šã‚¿ã‚°æ•°: {len(selected_tags)}")
            print(f"  ã‚¿ã‚°: {selected_tags[:5]}{'...' if len(selected_tags) > 5 else ''}")
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        final_result = {
            'stage': 2,
            'success': True,
            'results': results,
            'statistics': {
                'total_videos': len(all_video_data),
                'avg_tags_per_video': sum(len(r['selected_tags']) for r in results) / len(results) if results else 0,
                'total_tags_assigned': sum(len(r['selected_tags']) for r in results),
                'approved_candidates_used': len(approved_candidates),
                'processing_time': processing_time
            },
            'message': 'å…¨å‹•ç”»ã®ã‚¿ã‚°ä»˜ã‘ãŒå®Œäº†ã—ã¾ã—ãŸ'
        }
        
        print(f"\nç¬¬2æ®µéšå®Œäº†: {len(all_video_data)}ä»¶ã®å‹•ç”»ã‚’ã‚¿ã‚°ä»˜ã‘")
        print(f"å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")
        print(f"å¹³å‡ã‚¿ã‚°æ•°: {final_result['statistics']['avg_tags_per_video']:.1f}å€‹/å‹•ç”»")
        
        return final_result
    
    def _aggregate_non_transcript_data(self, all_video_data: List[Dict[str, Any]]) -> Dict[str, str]:
        """æ–‡å­—èµ·ã“ã—ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„"""
        print("æ–‡å­—èµ·ã“ã—ä»¥å¤–ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ä¸­...")
        
        titles = []
        skills = []
        descriptions = []
        summaries = []
        
        for video in all_video_data:
            # æ„å›³çš„ã« transcript ã¯é™¤å¤–
            if video.get('title'):
                titles.append(video['title'].strip())
            if video.get('skill'):
                skills.append(video['skill'].strip())
            if video.get('description'):
                descriptions.append(video['description'].strip())
            if video.get('summary'):
                summaries.append(video['summary'].strip())
        
        aggregated = {
            'all_titles': ' '.join(titles),
            'all_skills': ' '.join(skills),
            'all_descriptions': ' '.join(descriptions),
            'all_summaries': ' '.join(summaries)
        }
        
        print(f"  ã‚¿ã‚¤ãƒˆãƒ«: {len(titles)}ä»¶")
        print(f"  ã‚¹ã‚­ãƒ«: {len(skills)}ä»¶")
        print(f"  èª¬æ˜æ–‡: {len(descriptions)}ä»¶")
        print(f"  è¦ç´„: {len(summaries)}ä»¶")
        print(f"  â€»æ–‡å­—èµ·ã“ã—ã¯æ„å›³çš„ã«é™¤å¤–")
        
        return aggregated
    
    def _generate_tag_candidates(self, aggregated_data: Dict[str, str]) -> Set[str]:
        """é›†ç´„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¿ã‚°å€™è£œã‚’ç”Ÿæˆ"""
        
        # AIã‚’ä½¿ç”¨ã—ãŸã‚¿ã‚°å€™è£œç”Ÿæˆ
        if self.ai_handler:
            try:
                ai_candidates = self._generate_candidates_with_ai(aggregated_data)
                if ai_candidates:
                    print(f"AIåˆ†æã§{len(ai_candidates)}å€‹ã®å€™è£œã‚’ç”Ÿæˆ")
                    return set(ai_candidates)
            except Exception as e:
                print(f"AIåˆ†æã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†: {e}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        return self._extract_candidates_fallback(aggregated_data)
    
    def _generate_candidates_with_ai(self, aggregated_data: Dict[str, str]) -> List[str]:
        """AIåˆ†æã§ã‚¿ã‚°å€™è£œã‚’ç”Ÿæˆ"""
        
        # é«˜è² è·å¯¾ç­–: ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
        all_titles_text = aggregated_data['all_titles'][:2000] if aggregated_data['all_titles'] else ''
        all_skills_text = aggregated_data['all_skills'][:1000] if aggregated_data['all_skills'] else ''
        all_descriptions_text = aggregated_data['all_descriptions'][:3000] if aggregated_data['all_descriptions'] else ''
        all_summaries_text = aggregated_data['all_summaries'][:3000] if aggregated_data['all_summaries'] else ''
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚µã‚¤ã‚ºå‰Šæ¸›ãƒ­ã‚°
        print(f"  ãƒ†ã‚­ã‚¹ãƒˆé•·åˆ¶é™: ã‚¿ã‚¤ãƒˆãƒ«{len(all_titles_text)}, ã‚¹ã‚­ãƒ«{len(all_skills_text)}, èª¬æ˜{len(all_descriptions_text)}, è¦ç´„{len(all_summaries_text)}")
        
        prompt = f"""
ä»¥ä¸‹ã®å…¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ã€ã‚¿ã‚°å€™è£œã¨ãªã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‚‰ã¯å¾Œã§å€‹åˆ¥å‹•ç”»ã®è©³ç´°åˆ†æã§ä½¿ç”¨ã•ã‚Œã‚‹å€™è£œã§ã™ã€‚

ã€å…¨ã‚¿ã‚¤ãƒˆãƒ«é›†ç´„ã€‘:
{all_titles_text}

ã€å…¨ã‚¹ã‚­ãƒ«é›†ç´„ã€‘:
{all_skills_text}

ã€å…¨èª¬æ˜æ–‡é›†ç´„ã€‘:
{all_descriptions_text}

ã€å…¨è¦ç´„é›†ç´„ã€‘:
{all_summaries_text}

ã€ã‚¿ã‚°å€™è£œæŠ½å‡ºã®åŸºæº–ã€‘:
1. å…·ä½“çš„ãªãƒ„ãƒ¼ãƒ«åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒ»æ‰‹æ³•å
2. æ¥­ç•Œå›ºæœ‰ã®æ¦‚å¿µãƒ»ç†è«–ãƒ»ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
3. æ¸¬å®šå¯èƒ½ãªæŒ‡æ¨™åãƒ»KPI
4. å…·ä½“çš„ãªãƒ—ãƒ­ã‚»ã‚¹åãƒ»æ‰‹é †å
5. è·ç¨®ãƒ»æ¥­ç•Œãƒ»åˆ†é‡å

ã€çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ãæ±ç”¨èªã€‘:
- ã€Œ6ã¤ã®è¦ç´ ã€ã€Œ8ã¤ã®åˆ†é¡ã€ã€Œ4ã¤ã®ãƒã‚¤ãƒ³ãƒˆã€ç­‰ã®æ•°å­—+æ±ç”¨èª
- ã€Œè¦ç´ ã€ã€Œåˆ†é¡ã€ã€Œãƒã‚¤ãƒ³ãƒˆã€ã€Œæ‰‹æ³•ã€ã€Œæ–¹æ³•ã€ã€ŒæŠ€è¡“ã€ç­‰ã®å˜ä½“ä½¿ç”¨
- ã€ŒåŸºæœ¬ã€ã€Œå¿œç”¨ã€ã€Œå®Ÿè·µã€ã€Œç†è«–ã€ã€Œæ¦‚è¦ã€ã€Œå…¥é–€ã€ç­‰ã®æŠ½è±¡è¡¨ç¾

å‡ºåŠ›: å…·ä½“çš„ã§æœ‰ç”¨ãªã‚¿ã‚°å€™è£œã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        
        return self.ai_handler.call_openai(prompt)
    
    def _extract_candidates_fallback(self, aggregated_data: Dict[str, str]) -> Set[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        candidates = set()
        
        all_text = ' '.join(aggregated_data.values())
        
        # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        specific_patterns = [
            r'Google Analytics?', r'Salesforce', r'Instagram', r'Facebook', r'TikTok', r'YouTube', 
            r'Twitter', r'LinkedIn', r'ROI', r'CPA', r'CPM', r'CTR', r'LTV', r'CAC', r'ROAS', 
            r'KPI', r'OKR', r'PDCAã‚µã‚¤ã‚¯ãƒ«', r'PDCA', r'A/Bãƒ†ã‚¹ãƒˆ', r'SEO', r'SEM'
        ]
        
        for pattern in specific_patterns:
            matches = re.findall(pattern, all_text, re.IGNORECASE)
            for match in matches:
                if len(match) > 2:
                    candidates.add(match)
        
        # 2æ–‡å­—ä»¥ä¸Šã®æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        japanese_words = re.findall(r'[ã‚-ã‚“]+[ãƒ¼]?[ã‚-ã‚“]*|[ã‚¢-ãƒ³]+[ãƒ¼]?[ã‚¢-ãƒ³]*|[ä¸€-é¾¯]+', all_text)
        for word in japanese_words:
            if len(word) >= 3 and not self._is_generic_word(word):  # 3æ–‡å­—ä»¥ä¸Šã«åˆ¶é™
                candidates.add(word)
        
        return candidates
    
    def _apply_strict_generic_filter(self, tags: List[str]) -> List[str]:
        """å³æ ¼ãªæ±ç”¨ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if not tags:
            return tags
        
        # ã‚ˆã‚Šå³æ ¼ãªæ±ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©
        generic_patterns = [
            # æ•°å­—+æ±ç”¨èªãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå®Œå…¨ç¶²ç¾…ï¼‰
            r'\d+ã¤ã®è¦ç´ ', r'\d+ã¤ã®åˆ†é¡', r'\d+ã¤ã®ãƒã‚¤ãƒ³ãƒˆ', r'\d+ã¤ã®æ‰‹æ³•', r'\d+ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—',
            r'\d+ã¤ã®æ–¹æ³•', r'\d+ã¤ã®æŠ€è¡“', r'\d+ã¤ã®é …ç›®', r'\d+ã¤ã®è¦³ç‚¹', r'\d+ã¤ã®è¦–ç‚¹',
            r'\d+ã¤ã®åŸºæº–', r'\d+ã¤ã®åŸå‰‡', r'\d+ã¤ã®ç‰¹å¾´', r'\d+ã¤ã®æ®µéš', r'\d+ã¤ã®è¦å› ',
            r'\d+å€‹ã®è¦ç´ ', r'\d+å€‹ã®åˆ†é¡', r'\d+å€‹ã®ãƒã‚¤ãƒ³ãƒˆ', r'\d+å€‹ã®æ‰‹æ³•', r'\d+å€‹ã®ã‚¹ãƒ†ãƒƒãƒ—',
            r'\d+ã®è¦ç´ ', r'\d+ã®åˆ†é¡', r'\d+ã®ãƒã‚¤ãƒ³ãƒˆ', r'\d+ã®æ‰‹æ³•', r'\d+ã®ã‚¹ãƒ†ãƒƒãƒ—',
            r'\d+ã®æ–¹æ³•', r'\d+ã®æŠ€è¡“', r'\d+ã®é …ç›®', r'\d+ã®è¦³ç‚¹', r'\d+ã®è¦–ç‚¹',
            r'\d+ã®åŸºæº–', r'\d+ã®åŸå‰‡', r'\d+ã®ç‰¹å¾´', r'\d+ã®æ®µéš', r'\d+ã®è¦å› ', r'\d+ã®æ¡ä»¶',
            # æ•°å­—ã®ã¿+æ±ç”¨èª
            r'^\\d+è¦ç´ $', r'^\\d+åˆ†é¡$', r'^\\d+ãƒã‚¤ãƒ³ãƒˆ$', r'^\\d+æ‰‹æ³•$', r'^\\d+ã‚¹ãƒ†ãƒƒãƒ—$',
            r'^\\d+æ–¹æ³•$', r'^\\d+é …ç›®$', r'^\\d+æ®µéš$', r'^\\d+è¦³ç‚¹$', r'^\\d+è¦–ç‚¹$',
            # æ±ç”¨å˜èªï¼ˆå˜ä½“ï¼‰
            r'^è¦ç´ $', r'^åˆ†é¡$', r'^ãƒã‚¤ãƒ³ãƒˆ$', r'^æ‰‹æ³•$', r'^æ–¹æ³•$', r'^æŠ€è¡“$',
            r'^åŸºæœ¬$', r'^å¿œç”¨$', r'^å®Ÿè·µ$', r'^ç†è«–$', r'^æ¦‚è¦$', r'^å…¥é–€$',
            r'^åˆç´š$', r'^ä¸­ç´š$', r'^ä¸Šç´š$', r'^åŸºç¤$', r'^ç™ºå±•$', r'^æ´»ç”¨$',
            r'^ã‚¹ãƒ†ãƒƒãƒ—$', r'^æ®µéš$', r'^é …ç›®$', r'^è¦³ç‚¹$', r'^è¦–ç‚¹$', r'^æ¡ä»¶$',
            # æ±ç”¨ãƒ“ã‚¸ãƒã‚¹ç”¨èª
            r'^æ”¹å–„$', r'^æœ€é©åŒ–$', r'^å¼·åŒ–$', r'^å‘ä¸Š$', r'^æ¨é€²$', r'^å±•é–‹$',
            r'^æ§‹ç¯‰$', r'^ç¢ºç«‹$', r'^è¨­è¨ˆ$', r'^é‹ç”¨$', r'^ç®¡ç†$', r'^åˆ†æ$',
            r'^å®Ÿå‹™ã‚¹ã‚­ãƒ«$', r'^æ€è€ƒæ³•$', r'^æ¥­ç•ŒçŸ¥è­˜$', r'^ãƒ„ãƒ¼ãƒ«æ´»ç”¨$',
            r'^äººæè‚²æˆ$', r'^ã‚¹ã‚­ãƒ«é–‹ç™º$', r'^æˆæœå‘ä¸Š$', r'^åŠ¹ç‡åŒ–$'
        ]
        
        filtered = []
        removed_count = 0
        
        for tag in tags:
            tag = tag.strip()
            if not tag or len(tag) < 2:
                continue
            
            is_generic = False
            for pattern in generic_patterns:
                if re.match(pattern, tag):
                    print(f"    æ±ç”¨ã‚¿ã‚°ã‚’é™¤å¤–: '{tag}'")
                    is_generic = True
                    removed_count += 1
                    break
            
            if not is_generic:
                filtered.append(tag)
        
        print(f"  æ±ç”¨ã‚¿ã‚°ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°: {len(tags)}å€‹ â†’ {len(filtered)}å€‹ ({removed_count}å€‹é™¤å¤–)")
        return filtered
    
    def _apply_synonym_unification(self, tags: List[str]) -> List[str]:
        """é¡ç¾©èªçµ±ä¸€å‡¦ç†"""
        if not tags:
            return tags
        
        print("  é¡ç¾©èªçµ±ä¸€å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
        
        # é¡ç¾©èªè¾æ›¸ï¼ˆã‚ˆã‚Šé »å‡ºãƒ»æ¨™æº–çš„ãªè¡¨è¨˜ã«çµ±ä¸€ï¼‰
        synonym_groups = {
            # Google Analyticsé–¢é€£
            'Google Analytics': ['Google Analytics', 'GA', 'ã‚°ãƒ¼ã‚°ãƒ«ã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹', 'Googleã‚¢ãƒŠãƒªãƒ†ã‚£ã‚¯ã‚¹'],
            'Google Analytics 4': ['Google Analytics 4', 'GA4', 'GA 4', 'Google Analytics4'],
            'Google Tag Manager': ['Google Tag Manager', 'GTM', 'ã‚°ãƒ¼ã‚°ãƒ«ã‚¿ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼', 'Googleã‚¿ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼'],
            
            # Salesforceé–¢é€£  
            'Salesforce': ['Salesforce', 'SFDC', 'ã‚»ãƒ¼ãƒ«ã‚¹ãƒ•ã‚©ãƒ¼ã‚¹'],
            'Salesforce CRM': ['Salesforce CRM', 'SFDC CRM', 'ã‚»ãƒ¼ãƒ«ã‚¹ãƒ•ã‚©ãƒ¼ã‚¹CRM'],
            'Salesforce Einstein': ['Salesforce Einstein', 'Einstein AI', 'Einstein Analytics'],
            
            # åºƒå‘Šé–¢é€£
            'Facebookåºƒå‘Š': ['Facebookåºƒå‘Š', 'Facebook Ads', 'ãƒ•ã‚§ã‚¤ã‚¹ãƒ–ãƒƒã‚¯åºƒå‘Š', 'Metaåºƒå‘Š'],
            'Instagramåºƒå‘Š': ['Instagramåºƒå‘Š', 'Instagram Ads', 'ã‚¤ãƒ³ã‚¹ã‚¿ã‚°ãƒ©ãƒ åºƒå‘Š', 'IGåºƒå‘Š'],
            'Googleåºƒå‘Š': ['Googleåºƒå‘Š', 'Google Ads', 'AdWords', 'ã‚°ãƒ¼ã‚°ãƒ«åºƒå‘Š'],
            
            # Excelé–¢é€£
            'Excelé–¢æ•°': ['Excelé–¢æ•°', 'ã‚¨ã‚¯ã‚»ãƒ«é–¢æ•°', 'Excel Function', 'Excelæ•°å¼'],
            'VLOOKUPé–¢æ•°': ['VLOOKUPé–¢æ•°', 'VLOOKUP', 'ãƒ–ã‚¤ãƒ«ãƒƒã‚¯ã‚¢ãƒƒãƒ—', 'V LOOKUP'],
            'SUMIFSé–¢æ•°': ['SUMIFSé–¢æ•°', 'SUMIFS', 'ã‚µãƒ ã‚¤ãƒ•ã‚¹'],
            'INDEXé–¢æ•°': ['INDEXé–¢æ•°', 'INDEX', 'ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é–¢æ•°'],
            'MATCHé–¢æ•°': ['MATCHé–¢æ•°', 'MATCH', 'ãƒãƒƒãƒé–¢æ•°'],
            'ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«': ['ãƒ”ãƒœãƒƒãƒˆãƒ†ãƒ¼ãƒ–ãƒ«', 'PivotTable', 'ãƒ”ãƒœãƒƒãƒˆ', 'ã‚¯ãƒ­ã‚¹é›†è¨ˆ'],
            
            # åˆ†æãƒ»æ¸¬å®šé–¢é€£
            'A/Bãƒ†ã‚¹ãƒˆ': ['A/Bãƒ†ã‚¹ãƒˆ', 'ABãƒ†ã‚¹ãƒˆ', 'A/B Testing', 'ã‚¹ãƒ—ãƒªãƒƒãƒˆãƒ†ã‚¹ãƒˆ'],
            'ROIè¨ˆç®—': ['ROIè¨ˆç®—', 'ROI', 'æŠ•è³‡åç›Šç‡', 'Return on Investment'],
            'LTV': ['LTV', 'ãƒ©ã‚¤ãƒ•ã‚¿ã‚¤ãƒ ãƒãƒªãƒ¥ãƒ¼', 'é¡§å®¢ç”Ÿæ¶¯ä¾¡å€¤', 'Customer Lifetime Value'],
            'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡': ['ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡', 'CVR', 'Conversion Rate', 'æˆç´„ç‡'],
            'CTR': ['CTR', 'ã‚¯ãƒªãƒƒã‚¯ç‡', 'Click Through Rate', 'ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ç‡'],
            'CPA': ['CPA', 'ç²å¾—å˜ä¾¡', 'Cost Per Acquisition', 'Cost Per Action'],
            'CPM': ['CPM', 'ã‚¤ãƒ³ãƒ—ãƒ¬ãƒƒã‚·ãƒ§ãƒ³å˜ä¾¡', 'Cost Per Mille'],
            
            # SEOé–¢é€£
            'SEOå¯¾ç­–': ['SEOå¯¾ç­–', 'SEO', 'æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æœ€é©åŒ–', 'Search Engine Optimization'],
            'SEM': ['SEM', 'æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°', 'Search Engine Marketing'],
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ»æ‰‹æ³•é–¢é€£
            'KPI': ['KPI', 'é‡è¦æ¥­ç¸¾è©•ä¾¡æŒ‡æ¨™', 'Key Performance Indicator', 'ã‚­ãƒ¼ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™'],
            'OKR': ['OKR', 'Objectives and Key Results', 'ã‚ªãƒ¼ã‚±ãƒ¼ã‚¢ãƒ¼ãƒ«'],
            'PDCA': ['PDCA', 'PDCAã‚µã‚¤ã‚¯ãƒ«', 'Plan-Do-Check-Act'],
            'SWOTåˆ†æ': ['SWOTåˆ†æ', 'SWOT', 'ã‚¹ã‚¦ã‚©ãƒƒãƒˆåˆ†æ'],
            
            # CRMãƒ»å–¶æ¥­é–¢é€£
            'ãƒªãƒ¼ãƒ‰ç®¡ç†': ['ãƒªãƒ¼ãƒ‰ç®¡ç†', 'Lead Management', 'è¦‹è¾¼ã¿å®¢ç®¡ç†'],
            'å•†è«‡ç®¡ç†': ['å•†è«‡ç®¡ç†', 'Deal Management', 'Opportunity Management'],
            'é¡§å®¢ç®¡ç†': ['é¡§å®¢ç®¡ç†', 'Customer Management', 'CRM'],
            
            # ãã®ä»–ãƒ„ãƒ¼ãƒ«
            'Power BI': ['Power BI', 'PowerBI', 'ãƒ‘ãƒ¯ãƒ¼BI'],
            'Tableau': ['Tableau', 'ã‚¿ãƒ–ãƒ­ãƒ¼'],
            'BigQuery': ['BigQuery', 'ãƒ“ãƒƒã‚°ã‚¯ã‚¨ãƒª', 'Big Query']
        }
        
        # çµ±ä¸€ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ
        unification_map = {}
        for standard_term, variants in synonym_groups.items():
            for variant in variants:
                unification_map[variant] = standard_term
        
        # ã‚¿ã‚°ã‚’çµ±ä¸€
        unified_tags = []
        unification_count = 0
        
        for tag in tags:
            if tag in unification_map:
                unified_term = unification_map[tag]
                if unified_term not in unified_tags:
                    unified_tags.append(unified_term)
                    if tag != unified_term:
                        print(f"    é¡ç¾©èªçµ±ä¸€: '{tag}' â†’ '{unified_term}'")
                        unification_count += 1
            else:
                if tag not in unified_tags:
                    unified_tags.append(tag)
        
        print(f"  é¡ç¾©èªçµ±ä¸€å‡¦ç†: {len(tags)}å€‹ â†’ {len(unified_tags)}å€‹ ({unification_count}å€‹çµ±ä¸€)")
        return unified_tags
    
    def _validate_tags_against_candidates(self, ai_tags: List[str]) -> List[str]:
        """AIç”Ÿæˆã‚¿ã‚°ã‚’Stage1å€™è£œã«å¯¾ã—ã¦å³æ ¼ã«æ¤œè¨¼"""
        if not ai_tags or not self.approved_candidates:
            return []
        
        print(f"  ã‚¿ã‚°å€™è£œæ¤œè¨¼: AIç”Ÿæˆ{len(ai_tags)}å€‹ã‚’æ¤œè¨¼ä¸­...")
        
        # æ‰¿èªæ¸ˆã¿å€™è£œã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›ï¼ˆé«˜é€Ÿæ¤œç´¢ã®ãŸã‚ï¼‰
        approved_set = set(self.approved_candidates)
        
        validated_tags = []
        invalid_tags = []
        
        for tag in ai_tags:
            # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»ã—ã€æ”¹è¡Œç­‰ã®åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
            cleaned_tag = tag.strip().replace('\n', '').replace('\r', '')
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¿œç­”å½¢å¼å•é¡Œã‚’ä¿®æ­£ï¼ˆã€Œä»¥ä¸‹ã®ã‚¿ã‚°ã‚’é¸æŠã—ã¾ã—ãŸã€ç­‰ã®é™¤å»ï¼‰
            if cleaned_tag.startswith('ä»¥ä¸‹ã®') or cleaned_tag.startswith('é¸æŠã—ãŸ') or 'é¸æŠã—ã¾ã—ãŸ' in cleaned_tag:
                continue
            
            # ç©ºã®ã‚¿ã‚°ã‚„çŸ­ã™ãã‚‹ã‚¿ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not cleaned_tag or len(cleaned_tag) < 2:
                continue
            
            # Stage1å€™è£œã¨ã®å®Œå…¨ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            if cleaned_tag in approved_set:
                if cleaned_tag not in validated_tags:  # é‡è¤‡æ’é™¤
                    validated_tags.append(cleaned_tag)
            else:
                # éƒ¨åˆ†ä¸€è‡´ãƒã‚§ãƒƒã‚¯ï¼ˆé¡ä¼¼ã‚¿ã‚°ã®æ•‘æ¸ˆï¼‰
                partial_match = self._find_partial_match(cleaned_tag, approved_set)
                if partial_match:
                    if partial_match not in validated_tags:
                        validated_tags.append(partial_match)
                        print(f"    éƒ¨åˆ†ä¸€è‡´æ•‘æ¸ˆ: '{cleaned_tag}' â†’ '{partial_match}'")
                else:
                    invalid_tags.append(cleaned_tag)
        
        # æ¤œè¨¼çµæœã®ãƒ­ã‚°å‡ºåŠ›
        print(f"  æ¤œè¨¼çµæœ: {len(validated_tags)}å€‹æœ‰åŠ¹ã€{len(invalid_tags)}å€‹ç„¡åŠ¹")
        if invalid_tags:
            print(f"  ç„¡åŠ¹ã‚¿ã‚°ä¾‹: {invalid_tags[:5]}{'...' if len(invalid_tags) > 5 else ''}")
        
        # 15å€‹ã«æº€ãŸãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
        if len(validated_tags) < 15:
            shortage = 15 - len(validated_tags)
            remaining_candidates = [c for c in self.approved_candidates if c not in validated_tags]
            
            if remaining_candidates:
                # ãƒ©ãƒ³ãƒ€ãƒ ã§ã¯ãªãã€ãƒªã‚¹ãƒˆã®æœ€åˆã‹ã‚‰è¿½åŠ ï¼ˆä¸€è²«æ€§ã®ãŸã‚ï¼‰
                additional_tags = remaining_candidates[:shortage]
                validated_tags.extend(additional_tags)
                print(f"  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {len(additional_tags)}å€‹ã®ã‚¿ã‚°ã‚’è¿½åŠ ã—ã¦15å€‹ã«èª¿æ•´")
        
        return validated_tags[:15]  # å³æ ¼ã«15å€‹ã¾ã§
    
    def _find_partial_match(self, tag: str, approved_set: set) -> str:
        """éƒ¨åˆ†ä¸€è‡´ã§ã®ã‚¿ã‚°æ•‘æ¸ˆ"""
        tag_lower = tag.lower()
        
        # å®Œå…¨ä¸€è‡´ï¼ˆå¤§å°æ–‡å­—ç„¡è¦–ï¼‰
        for candidate in approved_set:
            if tag_lower == candidate.lower():
                return candidate
        
        # åŒ…å«é–¢ä¿‚ã§ã®ä¸€è‡´ï¼ˆçŸ­ã„ã‚¿ã‚°ãŒé•·ã„ã‚¿ã‚°ã«å«ã¾ã‚Œã‚‹å ´åˆï¼‰
        for candidate in approved_set:
            if tag_lower in candidate.lower() or candidate.lower() in tag_lower:
                # ãŸã ã—ã€é•·ã•ã®å·®ãŒå¤§ãã™ãã‚‹å ´åˆã¯é™¤å¤–
                if abs(len(tag) - len(candidate)) <= 3:
                    return candidate
        
        return None
    
    def _is_generic_word(self, word: str) -> bool:
        """æ±ç”¨èªãƒã‚§ãƒƒã‚¯"""
        generic_words = {
            'è¦ç´ ', 'åˆ†é¡', 'ãƒã‚¤ãƒ³ãƒˆ', 'æ‰‹æ³•', 'æ–¹æ³•', 'æŠ€è¡“', 'åŸºæœ¬', 'å¿œç”¨',
            'å®Ÿè·µ', 'ç†è«–', 'æ¦‚è¦', 'å…¥é–€', 'åˆç´š', 'ä¸­ç´š', 'ä¸Šç´š', 'åŸºç¤',
            'ç™ºå±•', 'æ´»ç”¨', 'ã«ã¤ã„ã¦', 'ã«ã‚ˆã‚‹', 'ãŸã‚ã®', 'ã¨ã¯', 'ã§ã™',
            'æ”¹å–„', 'æœ€é©åŒ–', 'å¼·åŒ–', 'å‘ä¸Š', 'æ¨é€²', 'å±•é–‹', 'æ§‹ç¯‰', 'ç¢ºç«‹',
            'è¨­è¨ˆ', 'é‹ç”¨', 'ç®¡ç†', 'åˆ†æ', 'ã‚¹ãƒ†ãƒƒãƒ—', 'æ®µéš', 'é …ç›®', 'è¦³ç‚¹', 'è¦–ç‚¹',
            'æ¡ä»¶', 'ç‰¹å¾´', 'è¦å› ', 'åŸºæº–', 'åŸå‰‡'
        }
        return word in generic_words
    
    def _analyze_individual_video(self, video_data: Dict[str, Any], ai_engine: str) -> List[str]:
        """å€‹åˆ¥å‹•ç”»ã®è©³ç´°åˆ†æï¼ˆæ–‡å­—èµ·ã“ã—å«ã‚€ï¼‰"""
        title = video_data.get('title', '')[:30]
        print(f"    åˆ†æä¸­: {title}...")
        
        if self.ai_handler:
            try:
                return self._ai_individual_analysis(video_data, ai_engine)
            except Exception as e:
                print(f"    AIåˆ†æã‚¨ãƒ©ãƒ¼ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: {e}")
        
        return self._fallback_individual_analysis(video_data)
    
    def _ai_individual_analysis(self, video_data: Dict[str, Any], ai_engine: str) -> List[str]:
        """AI ã«ã‚ˆã‚‹å€‹åˆ¥å‹•ç”»åˆ†æ"""
        transcript = video_data.get('transcript', '')
        transcript_excerpt = transcript[:2500] if transcript else ''
        
        candidates_str = ', '.join(sorted(list(self.approved_candidates)))
        
        prompt = f"""
ä»¥ä¸‹ã®å‹•ç”»ã«ã¤ã„ã¦ã€æ‰¿èªã•ã‚ŒãŸã‚¿ã‚°å€™è£œã‹ã‚‰æœ€ã‚‚é©åˆ‡ãªã‚¿ã‚°ã‚’10-15å€‹é¸æŠã—ã¦ãã ã•ã„ã€‚

ã€å‹•ç”»æƒ…å ±ã€‘:
ã‚¿ã‚¤ãƒˆãƒ«: {video_data.get('title', '')}
ã‚¹ã‚­ãƒ«: {video_data.get('skill', '')}
èª¬æ˜æ–‡: {video_data.get('description', '')}
è¦ç´„: {video_data.get('summary', '')}

ã€æ–‡å­—èµ·ã“ã—å†…å®¹ï¼ˆé‡è¦ï¼‰ã€‘:
{transcript_excerpt}

ã€æ‰¿èªæ¸ˆã¿ã‚¿ã‚°å€™è£œã€‘:
{candidates_str}

ã€é¸å®šåŸºæº–ï¼ˆå„ªå…ˆé †ä½é †ï¼‰ã€‘:
ğŸ¯ **æœ€å„ªå…ˆ: å·®åˆ¥åŒ–ã¨ç‰¹ç•°æ€§**
1. ã“ã®å‹•ç”»ã§ã—ã‹è¨€åŠã•ã‚Œãªã„å…·ä½“çš„ãªãƒ„ãƒ¼ãƒ«åãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒ»æ‰‹æ³•å
2. æ–‡å­—èµ·ã“ã—ã«ç™»å ´ã™ã‚‹å›ºæœ‰åè©ãƒ»æ•°å€¤ãƒ»å…·ä½“çš„äº‹ä¾‹
3. ä»–ã®é¡ä¼¼å‹•ç”»ã§ã¯æ‰±ã‚ã‚Œãªã„å°‚é–€çš„ãªæ¦‚å¿µãƒ»ç†è«–

ğŸ” **ç¬¬2å„ªå…ˆ: æ–‡å­—èµ·ã“ã—ç›´æ¥é–¢é€£æ€§**
4. æ–‡å­—èµ·ã“ã—ã§è¤‡æ•°å›è¨€åŠã•ã‚Œã‚‹é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
5. å‹•ç”»ã®æ ¸å¿ƒå†…å®¹ã‚’è¡¨ã™å°‚é–€ç”¨èª

âš¡ **ç¬¬3å„ªå…ˆ: å®Ÿç”¨æ€§ã¨æ¤œç´¢ä¾¡å€¤**
6. æ¤œç´¢æ™‚ã«æœ‰ç”¨ã§å…·ä½“æ€§ã®é«˜ã„ã‚¿ã‚°
7. å­¦ç¿’è€…ãŒæ±‚ã‚ã‚‹å®Ÿè·µçš„ãªçŸ¥è­˜ã‚’è¡¨ã™ã‚¿ã‚°

ã€çµ¶å¯¾ã«é¿ã‘ã‚‹ã¹ãæ±ç”¨ã‚¿ã‚°ï¼ˆå…·ä½“ä¾‹ï¼‰ã€‘:
âŒ ã€Œãƒ“ã‚¸ãƒã‚¹ã‚¹ã‚­ãƒ«ã€ã€Œãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã€ã€Œå–¶æ¥­ã€ã€Œã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€
âŒ ã€Œãƒ—ãƒ¬ã‚¼ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€ã€Œãƒªãƒ¼ãƒ€ãƒ¼ã‚·ãƒƒãƒ—ã€ã€Œãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆã€ã€Œæˆ¦ç•¥ã€
âŒ ã€Œåˆ†æã€ã€Œæ”¹å–„ã€ã€ŒåŠ¹ç‡åŒ–ã€ã€Œæœ€é©åŒ–ã€ã€Œå‘ä¸Šã€ã€Œå¼·åŒ–ã€
âŒ ã€ŒåŸºæœ¬ã€ã€Œå¿œç”¨ã€ã€Œå®Ÿè·µã€ã€Œç†è«–ã€ã€Œå…¥é–€ã€ã€Œæ¦‚è¦ã€
âŒ ã€Œã‚¹ã‚­ãƒ«é–‹ç™ºã€ã€Œäººæè‚²æˆã€ã€Œæ¥­å‹™æ”¹å–„ã€ã€Œçµ„ç¹”é‹å–¶ã€

ã€è‰¯ã„ã‚¿ã‚°ã®å…·ä½“ä¾‹ã€‘:
âœ… ã€ŒGoogle Analytics 4ã€ã€ŒSalesforce CRMã€ã€ŒInstagramåºƒå‘Šã€
âœ… ã€ŒROIè¨ˆç®—ã€ã€ŒA/Bãƒ†ã‚¹ãƒˆã€ã€Œã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡ã€ã€ŒLTVåˆ†æã€
âœ… ã€ŒPDCA ã‚µã‚¤ã‚¯ãƒ«ã€ã€ŒOKRè¨­å®šã€ã€ŒKPIè¨­è¨ˆã€ã€ŒSWOTåˆ†æã€
âœ… ã€ŒExcelé–¢æ•°ã€ã€ŒPower BIã€ã€ŒTableauã€ã€ŒSQL ã‚¯ã‚¨ãƒªã€

ã€å³å®ˆäº‹é …ã€‘:
- æ–°ã—ã„ã‚¿ã‚°ã¯ä½œæˆã›ãšã€æ‰¿èªæ¸ˆã¿å€™è£œã‹ã‚‰ã®ã¿é¸æŠ
- æ–‡å­—èµ·ã“ã—ã«å…¨ãé–¢é€£ã—ãªã„ã‚¿ã‚°ã¯çµ¶å¯¾ã«é¸æŠã—ãªã„
- æ±ç”¨çš„ã™ãã‚‹ã‚¿ã‚°ã¯å·®åˆ¥åŒ–ã®è¦³ç‚¹ã‹ã‚‰é™¤å¤–
- å¿…ãš15å€‹ã®ã‚¿ã‚°ã‚’é¸æŠï¼ˆ12-18å€‹ã®ç¯„å›²ã§èª¿æ•´å¯èƒ½ï¼‰

å‡ºåŠ›: é¸æŠã—ãŸã‚¿ã‚°ã®ã¿ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
        
        if ai_engine == 'openai':
            ai_tags = self.ai_handler.call_openai(prompt)
        elif ai_engine == 'claude':
            ai_tags = self.ai_handler.call_claude(prompt)
        elif ai_engine == 'gemini':
            ai_tags = self.ai_handler.call_gemini(prompt)
        else:
            ai_tags = []
        
        # å³æ ¼ãªã‚¿ã‚°æ¤œè¨¼: Stage1å€™è£œã‹ã‚‰ã®ã¿é¸æŠ
        validated_tags = self._validate_tags_against_candidates(ai_tags)
        return validated_tags
    
    def _fallback_individual_analysis(self, video_data: Dict[str, Any]) -> List[str]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€‹åˆ¥åˆ†æ"""
        content = f"{video_data.get('title', '')} {video_data.get('skill', '')} {video_data.get('description', '')} {video_data.get('summary', '')} {video_data.get('transcript', '')}".lower()
        
        selected = []
        for candidate in self.approved_candidates:
            if candidate.lower() in content and len(selected) < 15:
                selected.append(candidate)
        
        # æœ€ä½é™ã®æ•°ã‚’ç¢ºä¿
        if len(selected) < 15:
            remaining_candidates = [c for c in self.approved_candidates if c not in selected]
            for candidate in remaining_candidates[:15-len(selected)]:
                selected.append(candidate)
        
        return selected[:15]
    
    def _calculate_confidence(self, selected_tags: List[str], video_data: Dict[str, Any]) -> float:
        """ä¿¡é ¼åº¦è¨ˆç®—"""
        if not selected_tags:
            return 0.0
        
        # æ–‡å­—èµ·ã“ã—ã¨ã®é–¢é€£åº¦ã‚’ãƒã‚§ãƒƒã‚¯
        transcript = video_data.get('transcript', '').lower()
        if transcript:
            related_count = sum(1 for tag in selected_tags if tag.lower() in transcript)
            transcript_relevance = related_count / len(selected_tags)
        else:
            transcript_relevance = 0.5
        
        # ã‚¿ã‚°æ•°ã«ã‚ˆã‚‹ä¿¡é ¼åº¦
        tag_count_factor = min(1.0, len(selected_tags) / 15.0)
        
        # ç·åˆä¿¡é ¼åº¦
        confidence = (transcript_relevance * 0.7 + tag_count_factor * 0.3)
        return round(confidence, 2)